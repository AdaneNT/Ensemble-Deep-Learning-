{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Hybrid deep learning model for HARon  WISDM Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model_wisdm.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess data \n",
    "def preprocess(data):\n",
    "    #column_names = ['user_id','activity','timestamp', 'x_axis', 'y_axis', 'z_axis']\n",
    "    df = pd.read_csv(data,on_bad_lines='skip',index_col=False)\n",
    "    df.columns= ['user_id','activity','timestamp', 'x_axis', 'y_axis', 'z_axis']\n",
    "    df = df.dropna() # removing null values\n",
    "    df['z_axis'].replace(regex=True, inplace=True, to_replace=r';', value=r'') # Transforming the z-axis to float\n",
    "    \n",
    "    #### Label encoding \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    label = LabelEncoder()\n",
    "    df['label'] = label.fit_transform(df['activity'])\n",
    "    \n",
    "    ##Balanace this Data :Under sampling\n",
    "    Walking = df[df['activity']=='Walking'].head(48394).copy()\n",
    "    Jogging = df[df['activity']=='Jogging'].head(48394).copy()\n",
    "    Upstairs = df[df['activity']=='Upstairs'].head(48394).copy()\n",
    "    Downstairs = df[df['activity']=='Downstairs'].head(48394).copy()\n",
    "    Sitting = df[df['activity']=='Sitting'].head(48394).copy()\n",
    "    Standing = df[df['activity']=='Standing'].copy()\n",
    "    \n",
    "    #df_balanced = pd.DataFrame()\n",
    "    df_balanced = pd.concat([Walking, Jogging, Upstairs, Downstairs, Sitting, Standing])\n",
    "    \n",
    "    ##Normalize/Standardized data\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    X = df_balanced[['x_axis', 'y_axis', 'z_axis']]\n",
    "    target = df_balanced['label']\n",
    "    scaler = StandardScaler()\n",
    "    dx = scaler.fit_transform(X)\n",
    "    df_scaled = pd.DataFrame(data = dx, columns = X.columns)\n",
    "    df_scaled['label'] = target.values\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adanent\\Anaconda3\\envs\\Annotation_tool\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "dff=preprocess('C:/Datasets/WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt')\n",
    "#dff.head()#dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Segmenting samples (windowing) functions \n",
    "import scipy.stats as stats\n",
    "Fs = 20\n",
    "segment_size =  500# Fs*4 # window size=n_time steps=frame size\n",
    "step_size =  25 # Fs*2  # step size \n",
    "n_features = 3     \n",
    "\n",
    "def get_segments(df, segment_size, step_size):\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for i in range(0, len(df) - segment_size - 1, step_size):\n",
    "        xs = dff['x_axis'].values[i: i + segment_size]\n",
    "        ys = dff['y_axis'].values[i: i + segment_size]\n",
    "        zs = dff['z_axis'].values[i: i + segment_size]\n",
    "        label = stats.mode(dff['label'][i: i + segment_size])[0][0] # [0][0] shows the index of the current sequence\n",
    "        #We used the dstack() to ensure that each array is stacked in such a way that\n",
    "        #the features are separated in the third dimension, as we would prefer.\n",
    "        segments.append(np.dstack([xs, ys, zs]))\n",
    "        labels.append(label)\n",
    "\n",
    "    # Bring the segments into a better shape\n",
    "    segments=np.asarray(segments, dtype= np.float32).reshape(-1, segment_size, n_features)\n",
    "    labels = np.asarray(pd.get_dummies(labels), dtype = np.float32)\n",
    "                \n",
    "    return segments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_segments(dff, segment_size, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11595, 500, 3), (11595, 6))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-BiGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data into time steps of sub-sequences\n",
    "n_steps, n_length = 10, 50\n",
    "df_X = X.reshape((X.shape[0], n_steps, n_length, n_features))\n",
    "n_outputs = y.shape[1] #n_outputs= 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11595, 10, 50, 3), (11595, 6))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X.shape,y.shape,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define a CNN model that expects to read in sequences with a length of 32 time\n",
    "steps and three features. The entire CNN model can be wrapped in a TimeDistributed layer\n",
    "to allow the same CNN model to read in each of the four subsequences in the window. The\n",
    "extracted features are then flattened and provided to the LSTM model to read, extracting its\n",
    "own features before a final mapping to an activity is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM,Bidirectional,GRU\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training set and test set\n",
    "X_train_main, X_test, y_train_main, y_test = train_test_split(df_X, y,test_size=0.20, random_state=7)\n",
    "\n",
    "# split training set into training and validation set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_main,y_train_main,test_size=0.30,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6493, 10, 50, 3), (2319, 10, 50, 3), (6493, 6), (2319, 6))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer= 'rmsprop' , init= 'glorot_uniform' ):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'),input_shape=(None,n_length,n_features)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(Bidirectional(GRU(100)))  # Bidirectional LSTM\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize Mode\n",
    "cnn_Bigru_model=create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "130/130 [==============================] - 59s 267ms/step - loss: 0.5528 - accuracy: 0.7819 - val_loss: 0.1501 - val_accuracy: 0.9522\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 28s 216ms/step - loss: 0.0966 - accuracy: 0.9655 - val_loss: 0.0328 - val_accuracy: 0.9917\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 31s 237ms/step - loss: 0.0308 - accuracy: 0.9906 - val_loss: 0.0201 - val_accuracy: 0.9921\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 37s 281ms/step - loss: 0.0303 - accuracy: 0.9892 - val_loss: 0.0149 - val_accuracy: 0.9957\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 32s 243ms/step - loss: 0.0101 - accuracy: 0.9977 - val_loss: 0.0079 - val_accuracy: 0.9982\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 41s 316ms/step - loss: 0.0210 - accuracy: 0.9926 - val_loss: 0.0242 - val_accuracy: 0.9925\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 42s 325ms/step - loss: 0.0063 - accuracy: 0.9983 - val_loss: 0.0032 - val_accuracy: 0.9993\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 57s 442ms/step - loss: 0.0087 - accuracy: 0.9972 - val_loss: 0.0041 - val_accuracy: 0.9993\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 52s 403ms/step - loss: 0.0215 - accuracy: 0.9928 - val_loss: 0.0042 - val_accuracy: 0.9986\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 58s 448ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0021 - val_accuracy: 0.9996\n"
     ]
    }
   ],
   "source": [
    "history = cnn_Bigru_model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_Bigru_model.save('cnn_bilstm_500.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions on test \n",
    "y_pred_test = cnn_Bigru_model.predict(X_test) #class Prediction \n",
    "\n",
    "y_pred_test[:5], #y_probs_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test=np.argmax(y_pred_test,axis=1) \n",
    "y_true_test=np.argmax(y_test,axis=1) \n",
    "\n",
    "y_true_test.shape, y_pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import sklearn.metrics as skm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "cm=confusion_matrix(y_true_test,y_pred_test)\n",
    "print(cm)\n",
    "print(acc(y_true_test,y_pred_test))\n",
    "print(classification_report(y_true_test,y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf=np.empty((6,6))\n",
    "for i in range(6):\n",
    "    cf[i]=(cm[i]/np.sum(cm[i]))\n",
    "    \n",
    "# group_counts = [\"{0:.0f}\".format(value) for value in cm.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()]\n",
    "labels = [f\"{v1}\" for v1 in group_percentages]\n",
    "labels = np.asarray(labels).reshape(6,6)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax=fig.add_subplot(111)\n",
    "sns.heatmap(cm, annot=labels,fmt=\"\",cmap='RdYlBu_r',ax=ax)\n",
    "ax.set_xlabel(\"Predicted Labels\\n\\n Overall_accuracy=\"\"{0:.2%}\".format(acc(y_true_test,y_pred_test)))\n",
    "ax.set_ylabel(\"True Labels\")\n",
    "ax.set_title(\"Confusion Matrix\")\n",
    "ax.xaxis.set_ticklabels(['Walking','Jogging','Upstairs','Downstairs','Sitting','Standing'])\n",
    "ax.yaxis.set_ticklabels(['Walking','Jogging','Upstairs','Downstairs','Sitting','Standing'])\n",
    "plt.savefig('ConfusionMatrix_Ensem-HAR_WISDM',dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_true_test, y_pred_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
